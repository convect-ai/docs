{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Convect\u2122 AI Decision API is a collection of out-of-box APIs that supports the most common decision problems in Supply Chain Optimization. The goal is to let Convect\u2122 take care of the heavy-lifting part of building and tuning algorithms &amp; allocating computing resources, so you can load light and add intelligence easily when developing supply chain decision applications.</p> <p>Specifically, Convect\u2122 currently provides APIs to solve two types of problems:</p> <ul> <li>Automated forecasting</li> <li>Supply-demand planning</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Convect\u2122 implements the REST style APIs to enable users to activate models that solve decision problems programmatically. For a more comprehensive reference of the available APIs, please see reference.</p>"},{"location":"#obtaining-api-credentials","title":"Obtaining API credentials","text":"<ol> <li>Register on Convect Platform</li> <li>Go to the account settings page to obtain a pair of API <code>id</code> and <code>secrets</code>. Or contact Convect Support to obtain a demo key</li> <li>Once you have your <code>client_id</code> and <code>client_secret</code>, before calling the API, obtain an access token by sending a <code>POST</code> request as </li> </ol> <pre><code>curl --request POST \\\n--url https://forecast.convect.ai/api/auth/tokens \\\n--header 'content-type: application/json' \\\n--data '{\"client_id\":\"{YOUR_CLIENT_ID}\",\"client_secret\":\"{YOUR_CLIENT_SECRET}\",\"audience\":\"https://forecast.convect.ai\",\"grant_type\":\"client_credentials\"}'\n</code></pre> <p>and get a response</p> <pre><code>{\n\"access_token\": \"{RETURNED_TOKEN}\",\n  \"token_type\": \"Bearer\"\n}\n</code></pre> <ol> <li>Now you can attach the token as specified in <code>access_token</code> field in the following API calls.</li> </ol>"},{"location":"#calling-the-api","title":"Calling the API","text":"<ul> <li> <p>Attach the returned token in every header of the requests you send to a Convect\u2122 API. For example, set <code>Authorization: Bearer {RETURNED_TOKEN}</code> in the header.</p> </li> <li> <p>Convect\u2122 APIs also accept and return <code>json</code> type data, so you can set <code>Accept: application/json</code> and <code>Content-type: application/json</code> in your header as well.</p> </li> </ul>"},{"location":"#example-automated-forecasting","title":"Example - Automated Forecasting","text":"<p>Here is an example of training and generating forecasts by calling the automated forecasting APIs.</p> python <pre><code>#!/bin/env python\n\nimport requests\nimport os.path\nimport pandas as pd\nfrom time import sleep\n\nBASE_ENDPOINT = \"https://forecast.convect.ai/api/\"\nDEMO_KEY = \"YOUR_API_KEY\"\n\nHEADER = {\n    \"Accept\": \"application/json\",\n    \"Content-type\": \"application/json\",\n    \"Authorization\": \"Bearer {}\".format(DEMO_KEY),\n}\n\n# create the data group\n\npayload = {\"name\": \"Demo data group\"}\nresp = requests.post(\n    os.path.join(BASE_ENDPOINT, \"data-groups/\"),\n    json=payload,\n    headers=HEADER,\n)\ndata_group_id = resp.json()[\"id\"]\n\n# upload the dataset\ndata_url = \"https://convect-test-data.s3.us-west-2.amazonaws.com/forecast_test_data/target_ts.csv\"\npayload = {\n    \"name\": \"target time series\",\n    \"dataset_type\": \"TARGET_TIME_SERIES\",\n    \"path\": data_url,\n    \"file_format\": \"csv\",\n    \"frequency\": \"W\",\n    \"data_group\": data_group_id,\n    \"schemas\": [\n        {\"name\": \"sku\", \"col_type\": \"key\"},\n        {\"name\": \"week\", \"col_type\": \"time\"},\n        {\"name\": \"qty\", \"col_type\": \"num\"},\n    ],\n}\nresp = requests.post(\n    os.path.join(BASE_ENDPOINT, \"datasets/\"),\n    json=payload,\n    headers=HEADER,\n)\ndataset_id = resp.json()[\"id\"]\n\n\n# build the forecating config\nurl = os.path.join(BASE_ENDPOINT, \"predictor-configs/\")\noutput_path = \"s3://convect-data/result/demo-run\"\npayload = {\n    \"name\": \"14 week forecast config\",\n    \"result_uri\": output_path,\n    \"horizon\": 14,\n    \"frequency\": \"W\",\n    \"data_group\": data_group_id,\n}\n\nresp = requests.post(\n    os.path.join(BASE_ENDPOINT, \"predictor-configs/\"),\n    json=payload,\n    headers=HEADER,\n)\nconfig_id = resp.json()[\"id\"]\n\n# trigger the forecasting\npayload = {\"predictor_config\": config_id}\nresp = requests.post(\n    os.path.join(BASE_ENDPOINT, \"predictors/\"),\n    json=payload,\n    headers=HEADER,\n)\npredictor_id = resp.json()[\"id\"]\n\n# query the result\nresp = requests.get(\n    os.path.join(\n        BASE_ENDPOINT, \"predictors/\", f\"{predictor_id}/\"\n    ),\n    headers=HEADER,\n)\nprint(resp.json())\n\n# wait while the run finishes\nwhile True:\n    resp = requests.get(\n        os.path.join(\n            BASE_ENDPOINT, \"predictors/\", f\"{predictor_id}/\"\n        ),\n        headers=HEADER,\n    )\n    status = resp.json()[\"status\"][\"status\"]\n    if status == \"Running\":\n        sleep(10)\n    else:\n        break\n\n# retrieve the result\ndf = pd.read_csv(output_path)\n</code></pre> curl <pre><code>#!/bin/env bash\nset -e\n\n# an example of how to use the convect forecast API to trigger a forecast run\n\n# fetch the token \nexport CLIENT_ID=\"&lt;YOUR CLIENT ID HERE&gt;\"\nexport CLIENT_SECRET='&lt;YOUR CLIENT SECRET HERE&gt;'\nexport BASE_URL='https://forecast.convect.ai/api'\n\nTOKEN=$(curl -s --request POST $BASE_URL/auth/tokens \\\n-H 'Content-Type: application/json' \\\n--data-binary @- &lt;&lt; EOF | jq -r '.access_token'\n    {\n        \"client_id\": \"${CLIENT_ID}\",\n        \"client_secret\": \"${CLIENT_SECRET}\",\n        \"audience\": \"https://forecast.convect.ai\",\n        \"grant_type\": \"client_credentials\"\n    }\nEOF\n)\n\necho \"token: ${TOKEN}\"\n\nexport AUTH_HEADER=\"Authorization: Bearer ${TOKEN}\"\n\n# create a dataset group\ndata_group_id=$(curl -s --request POST $BASE_URL/data-groups/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" \\\n--data '{\"name\": \"Demo data group\"}' | jq '.id')\n\n# information about the dataset group\ncurl -s --request GET $BASE_URL/data-groups/${data_group_id}/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" | jq\n\n# upload a dataaset\nexport data_url=\"https://convect-test-data.s3.us-west-2.amazonaws.com/forecast_test_data/target_ts.csv\"\ncurl -s --request POST $BASE_URL/datasets/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" \\\n--data-binary @- &lt;&lt; EOF\n    {\n        \"name\": \"target time series\",\n        \"dataset_type\": \"TARGET_TIME_SERIES\",\n        \"path\": \"${data_url}\",\n        \"file_format\": \"csv\",\n        \"frequency\": \"W\",\n        \"data_group\": ${data_group_id},\n        \"schemas\": [\n            {\"name\": \"sku\", \"col_type\": \"key\"},\n            {\"name\": \"week\", \"col_type\": \"time\"},\n            {\"name\": \"qty\", \"col_type\": \"num\"}\n        ]\n    }\nEOF\n\n\n# set up a forecat config\nexport output_path='s3://convect-data/result/demo-run'\n\nconfig_id=$(curl -s --request POST $BASE_URL/predictor-configs/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" \\\n--data-binary @- &lt;&lt; EOF | jq '.id'\n    {\n        \"name\": \"12 week forecast config\",\n        \"result_uri\": \"${output_path}\",\n        \"horizon\": 14,\n        \"frequency\": \"W\",\n        \"data_group\": ${data_group_id}\n    }\nEOF\n)\n\necho \"config id: ${config_id}\"\n\n\n# trigger a forecast run\nrun_id=$(curl -s --request POST $BASE_URL/predictors/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" \\\n--data-binary @- &lt;&lt; EOF | jq '.id' \n    {\n        \"predictor_config\": ${config_id}\n    }\nEOF\n)\n\necho \"run id: ${run_id}\"\n\n\n# query the run status until it is done, or failed, or timed out\ntimeout=600\nwhile [ $timeout -gt 0 ]; do\njob_status=$(curl -s --request GET $BASE_URL/predictors/${run_id}/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" | jq '.status.status')\necho \"status: ${job_status}\"\nif [ \"$job_status\" == '\"Succeeded\"' ]; then\nbreak\nelif [ \"$job_status\" == '\"Failed\"' ]; then\nbreak\nfi\nsleep 10\ntimeout=$((timeout-10))\ndone\n\n\n# retrieve the result\nresult_uri=$(curl -s --request GET $BASE_URL/predictors/${run_id}/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" | jq -r '.result_uri')\n\necho \"result uri: ${result_uri}\"\n\n# download the result as result.csv\ncurl -fsSL ${result_uri} -o result.csv\n\n# check the result\nhead -n 10 result.csv\n</code></pre>"},{"location":"flowopt/data-prepare/","title":"Data preparation","text":"<p>WIP</p>"},{"location":"forecast/data-prepare/","title":"Prepare the data","text":""},{"location":"forecast/data-prepare/#dataset-types-supported","title":"Dataset Types Supported","text":"<p>Convect\u2122 Forecast supports the following types of datasets:</p> <ul> <li>Target time series</li> <li>Related time series </li> <li>Item meta information </li> </ul> <p>Target time series data is the only dataset type needed to build a forecasting model. The dataset records the time series values for an entity at different historical timestamps. For example, for an retailer, the data captures the unit sold for each product at different dates.</p> <p>Related time series data captures time-varying values that are not direct targets to be forecasted associated with entities. For example, the data may record the price of a product on each date. </p> <p>Item meta information provides additional information static across time about the entity to be forecasted. For example, the data may contain categorical information such as brands, categories, and vendors of entities.</p>"},{"location":"forecast/data-prepare/#data-formats","title":"Data Formats","text":"<p>All 3 types of datasets require one or multiple columns that indicate the identifiers of entities. For example, for an retailer, these columns may include a SKU plus a store id that uniquely define the identity of an entity to be forecasted.</p>"},{"location":"forecast/data-prepare/#target-time-series","title":"Target time series","text":"<p>The dataset requires at least 3 columns:</p> <ul> <li>1 or more columns that serve as the key column(s) mentioned above</li> <li>1 timestamp column that marks when the observation was recorded</li> <li>1 value column that stores the actual value of the time series</li> </ul> <p>For example</p> model week qty 1163704 2017-01-01 267 1163704 2017-01-08 229 5998369 2017-01-01 1689 5998369 2017-01-08 1322 <p>where <code>model</code> is the key column of the entity; <code>week</code> is the timestamp column; <code>qty</code> stores the actual time series value.</p>"},{"location":"forecast/data-prepare/#related-time-series","title":"Related time series","text":"<p>Similarly to target time series data, the dataset requires at least 3 columns: </p> <ul> <li>1 or more columns that serve as the key column(s) mentioned above</li> <li>1 timestamp column that marks when the observation was recorded</li> <li>1 or more value columns that store the actual values of the related time series values</li> </ul> <p>For example</p> model week price temperature 1163704 2017-01-01 12.5 20.5 1163704 2017-01-08 12.3 22.3 5998369 2017-01-01 5.6 20.5 5998369 2017-01-08 4.5 22.3 <p>where <code>model</code> is the key column of the entity; <code>week</code> is the timestamp column; <code>price</code> and <code>temperature</code> store two related time series values about the entity.</p>"},{"location":"forecast/data-prepare/#item-meta-information","title":"Item meta information","text":"<p>Item meta information requires at least 2 columns:</p> <ul> <li>1 or more columns that serve as the key column(s) mentioned above</li> <li>1 or more value columns that store some meta information about the entity</li> </ul> <p>For example</p> model brand category 1163704 XXX Food 5998369 YYY Pet <p>where <code>model</code> is the key column of the entity; <code>brand</code> and <code>category</code> are the meta information associated to each entity.</p> <p>Note</p> <p>Item meta information dataset does not require a timestamp column as target and related time series datasets.</p>"},{"location":"forecast/data-prepare/#calling-datagroup-apis","title":"Calling Datagroup APIs","text":"<p>Once you have the datasets prepared in the described formats, the first step toward building a model on top is to declare those datasets by calling the <code>datagroup</code> APIs.</p>"},{"location":"forecast/data-prepare/#upload-the-datasets","title":"Upload the datasets","text":"<p>Before calling the APIs, it's better to make datasets available as remote urls. To do so, there are multiple options:</p> <ul> <li>Upload to an Object Storage such as S3, Google Cloud Storage and make the file available to be read (by either making it public or pre-signing it)</li> <li>Upload to a share drive such as Dropbox, Google Drive and generate a sharing url</li> <li>Self host it at a file server (e.g, samba)</li> </ul>"},{"location":"forecast/data-prepare/#declare-datasets-using-api","title":"Declare Datasets Using API","text":"<p>Step1. Create a <code>datagroup</code></p> <p><code>datagroups</code> serves as the container to host multiple datasets that are related. You can give it whatever name that makes sense to you.</p> python <pre><code>#!/bin/env python\n\nimport requests\nimport os.path\nimport pandas as pd\nfrom time import sleep\n\nBASE_ENDPOINT = \"https://forecast.convect.ai/api/\"\nDEMO_KEY = \"YOUR_API_KEY\"\n\nHEADER = {\n    \"Accept\": \"application/json\",\n    \"Content-type\": \"application/json\",\n    \"Authorization\": \"Bearer {}\".format(DEMO_KEY),\n}\n\n# create the data group\n\npayload = {\"name\": \"Demo data group\"}\nresp = requests.post(\n    os.path.join(BASE_ENDPOINT, \"data-groups/\"),\n    json=payload,\n    headers=HEADER,\n)\ndata_group_id = resp.json()[\"id\"]\n</code></pre> curl <pre><code>#!/bin/env bash\nset -e\n\n# an example of how to use the convect forecast API to trigger a forecast run\n\n# fetch the token \nexport CLIENT_ID=\"&lt;YOUR CLIENT ID HERE&gt;\"\nexport CLIENT_SECRET='&lt;YOUR CLIENT SECRET HERE&gt;'\nexport BASE_URL='https://forecast.convect.ai/api'\n\nTOKEN=$(curl -s --request POST $BASE_URL/auth/tokens \\\n-H 'Content-Type: application/json' \\\n--data-binary @- &lt;&lt; EOF | jq -r '.access_token'\n    {\n        \"client_id\": \"${CLIENT_ID}\",\n        \"client_secret\": \"${CLIENT_SECRET}\",\n        \"audience\": \"https://forecast.convect.ai\",\n        \"grant_type\": \"client_credentials\"\n    }\nEOF\n)\n\necho \"token: ${TOKEN}\"\n\nexport AUTH_HEADER=\"Authorization: Bearer ${TOKEN}\"\n\n# create a dataset group\ndata_group_id=$(curl -s --request POST $BASE_URL/data-groups/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" \\\n--data '{\"name\": \"Demo data group\"}' | jq '.id')\n\n# information about the dataset group\ncurl -s --request GET $BASE_URL/data-groups/${data_group_id}/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" | jq\n</code></pre> <p>Step2. Declare target time series <code>datasets</code> under a <code>datagroup</code></p> <p>The next step is to associate individual <code>dataset</code> object to a <code>datagroup</code>. When declaring the <code>dataset</code>, users also need to provide the schema about the dataset, i.e., names of the columns and their roles. For example, below code associate a target time series <code>dataset</code> to a <code>datagroup</code>.</p> python <pre><code># upload the dataset\ndata_url = \"https://convect-test-data.s3.us-west-2.amazonaws.com/forecast_test_data/target_ts.csv\"\npayload = {\n    \"name\": \"target time series\",\n    \"dataset_type\": \"TARGET_TIME_SERIES\",\n    \"path\": data_url,\n    \"file_format\": \"csv\",\n    \"frequency\": \"W\",\n    \"data_group\": data_group_id,\n    \"schemas\": [\n        {\"name\": \"sku\", \"col_type\": \"key\"},\n        {\"name\": \"week\", \"col_type\": \"time\"},\n        {\"name\": \"qty\", \"col_type\": \"num\"},\n    ],\n}\nresp = requests.post(\n    os.path.join(BASE_ENDPOINT, \"datasets/\"),\n    json=payload,\n    headers=HEADER,\n)\ndataset_id = resp.json()[\"id\"]\n</code></pre> curl <pre><code># upload a dataaset\nexport data_url=\"https://convect-test-data.s3.us-west-2.amazonaws.com/forecast_test_data/target_ts.csv\"\ncurl -s --request POST $BASE_URL/datasets/ \\\n    -H 'Content-Type: application/json' \\\n    -H \"$AUTH_HEADER\" \\\n    --data-binary @- &lt;&lt; EOF\n    {\n        \"name\": \"target time series\",\n        \"dataset_type\": \"TARGET_TIME_SERIES\",\n        \"path\": \"${data_url}\",\n        \"file_format\": \"csv\",\n        \"frequency\": \"W\",\n        \"data_group\": ${data_group_id},\n        \"schemas\": [\n            {\"name\": \"sku\", \"col_type\": \"key\"},\n            {\"name\": \"week\", \"col_type\": \"time\"},\n            {\"name\": \"qty\", \"col_type\": \"num\"}\n        ]\n    }\nEOF\n</code></pre> <ul> <li><code>dataset_type</code> specifies the type of the dataset. Available options are <code>TARGET_TIME_SERIES</code>, <code>RELATED_TIME_SERIES</code>, and <code>ITEM_META</code>.</li> <li><code>path</code> points to the remote url of the file uploaded. Here we use S3 as the file storage.</li> <li><code>frequency</code> specifies the frequency the time series was recorded.</li> <li><code>schemas</code> is a list of dict. Each entry contains <code>name</code> of the column, <code>col_type</code> specifies the type and role of the column.</li> </ul> <p>Step3. (Optional) Declare more dataset types under the data group.</p> <p>Similarly, you can declare more individual datasets, such as related time series and item meta information. Below is an example of declaring an item meta information</p> <pre><code>url = os.path.join(endpoint, \"datasets/\")\n\npayload = {\n    \"name\": f\"item_meta_{store_id}\",\n    \"dataset_type\": \"ITEM_METADATA\",\n    \"path\": get_meta_path(),\n    \"file_format\": \"csv\",\n    \"data_group\": data_group_id,\n    \"schemas\": [\n        {\"name\": \"id\", \"col_type\": \"key\"},\n        {\"name\": \"dept_id\", \"col_type\": \"str\"},\n        {\"name\": \"cat_id\", \"col_type\": \"str\"},\n    ],\n}\n\nresp = requests.post(url, json=payload)\nresp_payload = resp.json()\nprint(resp_payload)\n\nreturn resp_payload[\"id\"]\n</code></pre>"},{"location":"forecast/overview/","title":"Overview","text":""},{"location":"forecast/overview/#background","title":"Background","text":"<p>Convect\u2122 Forecast provides the tool to automate building and tuning of a forecast model. </p> <p>Forecasting serves as the prerequisite of many decision tasks. For example, when one decides the right inventory level for a product, it's critical to have some insights on the number of units predicted to be sold in the future and the uncertainty about the prediction (which is commonly neglected when people are talking about building predictive models).</p> <p>Another example is the need to know how soon a vendor can deliver a batch of orders to your warehouse locations (lead time) when planning for the inventory level. Longer lead time means more uncertainty and higher safety stock levels.</p> <p>Therefore, having a way to quickly build a forecast model is critical when developing such decision applications as it helps you move to building the prescriptive modeling faster.</p>"},{"location":"forecast/overview/#features","title":"Features","text":"<p>Compared to traditional time series modeling, Convect\u2122 Forecast excels at handling the following problems:</p> <ul> <li>Multivariate time series</li> <li>Multiple time series</li> </ul> <p>Multivariate time series forecasting problems tackle the situation in which there are other features besides the time series itself can provide signals in predicting the future values. This is quite common in e.g., retail settings where information about the SKU, such as brand, category, can help to predict the future demands. </p> <p>Traditional methods that are widely used in ERP systems (such as SAP) and spreadsheet-style forecasting workflows, such that Exponential Smoothing and ARIMA are known to be lacking in providing this feature.</p> <p>Multiple time series forecasting problems tackle the situation in which many correlated time series need to be forecasted with one shot. This is also common in e.g., the retail setting. Product demands correlate with each other, e.g., in the famous bear and diaper case, which can lead to higher accuracy if such information is utilized when building the model.</p> <p>Popular forecasting library such as Facebook Prophet is known to lacking support for this feature.</p>"},{"location":"forecast/overview/#what-does-convecttm-forecast-do-for-you","title":"What does Convect\u2122 Forecast do for you","text":"<p>Convect\u2122 Forecast eases the process of building forecasting models for multivariate, multiple time series problems. Specifically, it does the following things for users:</p> <ul> <li>Preprocess data to make it ready for model building</li> <li>Generate features to be used by models</li> <li>Augment data to increase model accuracy</li> <li>Choose the right strategy and level to build models on</li> <li>Choose the strategy to evaluate model candidates against history data</li> <li>Select the best model across a large pool of candidate models</li> <li>Package the data processing and modeling process as portable bundle so it can be deployed to make predictions on unseen data</li> </ul>"},{"location":"forecast/overview/#workflow","title":"Workflow","text":"<p>To build a model using Convect\u2122 Forecast, we need to go through the following steps:</p> <ul> <li>Prepare data according to the given format</li> <li>Gain insights about the model performance on history data (Optional)</li> <li>Build and generate forecast</li> </ul>"},{"location":"forecast/run-backtest/","title":"Evaluating the model","text":""},{"location":"forecast/run-backtest/#what-is-backtesting","title":"What is backtesting","text":"<p>Backtesting is a technique to go back to the past and generate a forecast result at a cutoff date, then compare the result with the actual observations to compute the accuracy metrics of a model.</p> <p>For example, if I would like to generate the sales forecast for a store during the coming Thanksgiving season, a good backtesting strategy is to go back 1 year, to a date prior to Thanksgiving, and to generate a forecast result. Then compare the result with last year's Thanksgiving sales to gain an understanding how my model is likely to perform for this year based on last year's performance.</p>"},{"location":"forecast/run-backtest/#set-up-a-backtesting","title":"Set up a backtesting","text":"<p>Before setting up a backtesting experiment, we will go through the same process of preparing data and constructing a <code>predictor-config</code>.</p> <p>Once you have the <code>predictor-config</code>, we first create a <code>backtest-config</code> by</p> <pre><code>import requests\nimport os.path\n\nBASE_ENDPOINT = \"https://forecast.convect.ai/api/\"\nDEMO_KEY = \"YOUR_API_KEY\"\n\nHEADER = {\n    \"Accept\": \"application/json\",\n    \"Content-type\": \"application/json\",\n    \"Authorization\": \"Bearer {}\".format(DEMO_KEY),\n}\n\npayload = {\n    backtest_dates: [\"2019-11-10\", \"2019-12-20\"],\n    name: \"holiday sales backtest\",\n    result_uri: \"s3://convect-data/result/backtest-results/\",\n    predictor: YOUR_PREDICTOR_ID\n}\n\nresp = requests.post(\n    os.path.join(BASE_ENDPOINT, \"backtest-configs/\"),\n    json=payload,\n    headers=HEADER\n)\n\nconfig_id = resp.json()[\"id]\n</code></pre> <p>Then we trigger the actual runs based on the config by</p> <pre><code>payload = {\"backtest_config\": config_id}\nresp = requests.post(\n    os.path.join(BASE_ENDPOINT, \"backtesters/\"),\n    json=payload,\n    headers=HEADER\n)\njob_id = resp.json()[\"id]\n</code></pre> <p>And you can query the status of the run similarly to the normal forecast runs by </p> <pre><code>resp = requests.get(\n    os.path.join(BASE_ENDPOINT, \"backtesters\", f\"{job_id}/\"),\n    headers=HEADER\n)\nprint(resp.json())\n</code></pre>"},{"location":"forecast/run-forecast/","title":"Running the forecast","text":""},{"location":"forecast/run-forecast/#forecast-configuration","title":"Forecast Configuration","text":"<p>Once you finish declaring all the <code>datasets</code> associated with a <code>datagroup</code>, you can proceed to set up a forecast configuration.</p> <p>The configuration defines the forecast task, e.g., how long should the forecast horizon be, what metrics to use when evaluating models, and if uncertainty interval should be included in the results, where to write the result files.</p> <p>For example, code below sets up a task that outputs the consecutive 14-week predicted value starting from the last date in the dataset.</p> python <pre><code># build the forecating config\nurl = os.path.join(BASE_ENDPOINT, \"predictor-configs/\")\noutput_path = \"s3://convect-data/result/demo-run\"\npayload = {\n    \"name\": \"14 week forecast config\",\n    \"result_uri\": output_path,\n    \"horizon\": 14,\n    \"frequency\": \"W\",\n    \"data_group\": data_group_id,\n}\n\nresp = requests.post(\n    os.path.join(BASE_ENDPOINT, \"predictor-configs/\"),\n</code></pre> curl <pre><code># set up a forecat config\nexport output_path='s3://convect-data/result/demo-run'\n\nconfig_id=$(curl -s --request POST $BASE_URL/predictor-configs/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" \\\n--data-binary @- &lt;&lt; EOF | jq '.id'\n    {\n        \"name\": \"12 week forecast config\",\n        \"result_uri\": \"${output_path}\",\n        \"horizon\": 14,\n        \"frequency\": \"W\",\n        \"data_group\": ${data_group_id}\n    }\nEOF\n)\n\necho \"config id: ${config_id}\"\n</code></pre>"},{"location":"forecast/run-forecast/#trigger-forecast","title":"Trigger Forecast","text":"<p>Once a config is set up, we can trigger the actual forecast run by calling <code>POST</code> endpoint <code>predictors/</code> by providing the id of the config.</p> python <pre><code>)\nconfig_id = resp.json()[\"id\"]\n\n# trigger the forecasting\npayload = {\"predictor_config\": config_id}\n</code></pre> curl <pre><code># trigger a forecast run\nrun_id=$(curl -s --request POST $BASE_URL/predictors/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" \\\n--data-binary @- &lt;&lt; EOF | jq '.id' \n    {\n        \"predictor_config\": ${config_id}\n    }\nEOF\n)\n\necho \"run id: ${run_id}\"\n</code></pre>"},{"location":"forecast/run-forecast/#retrieve-result","title":"Retrieve Result","text":"<p>Once a run is triggered, query the status of the run by calling <code>GET</code> endpoint <code>predictors/{id}</code>.</p> python <pre><code>    json=payload,\n    headers=HEADER,\n)\npredictor_id = resp.json()[\"id\"]\n\n# query the result\n</code></pre> curl <pre><code># query the run status until it is done, or failed, or timed out\ntimeout=600\nwhile [ $timeout -gt 0 ]; do\njob_status=$(curl -s --request GET $BASE_URL/predictors/${run_id}/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" | jq '.status.status')\necho \"status: ${job_status}\"\nif [ \"$job_status\" == '\"Succeeded\"' ]; then\nbreak\nelif [ \"$job_status\" == '\"Failed\"' ]; then\nbreak\nfi\nsleep 10\ntimeout=$((timeout-10))\ndone\n\n\n# retrieve the result\nresult_uri=$(curl -s --request GET $BASE_URL/predictors/${run_id}/ \\\n-H 'Content-Type: application/json' \\\n-H \"$AUTH_HEADER\" | jq -r '.result_uri')\n\necho \"result uri: ${result_uri}\"\n\n# download the result as result.csv\ncurl -fsSL ${result_uri} -o result.csv\n\n# check the result\nhead -n 10 result.csv\n</code></pre> <p>Once the returned <code>status</code> is indicated as <code>Succeeded</code>, you can read the result from the output path specified when setting up the forecast config.</p> <p>Below is an example of the result file</p> key sku predict_sum predict_start_date predict_horizon 11000015 11000015 1.000000027104373 2021-08-31 7 11000016 11000016 3.000000027104373 2021-08-31 7 11000017 11000017 1.000000027104373 2021-08-31 7 11000018 11000018 1.0577409169104601 2021-08-31 7 11000019 11000019 0.998951528482072 2021-08-31 7 11000020 11000020 1.0051190404354284 2021-08-31 7 11000021 11000021 0.9999988959151295 2021-08-31 7"},{"location":"tutorials/custom-pipeline/","title":"Custom forecasting pipeline","text":"<p>WIP</p>"},{"location":"tutorials/m5/","title":"M5 forecasting","text":""},{"location":"tutorials/m5/#background","title":"Background","text":"<p>This tutorial walks you through how to utilize Convect\u2122 Automated Forecasting API to finish the M5 Forecasting challenge.</p>"},{"location":"tutorials/m5/#prepare-data","title":"Prepare Data","text":"<p>Because the M5 data is in the wide format, we first need to convert them to the regular format as specified in data preparation section.</p> <pre><code>import pandas as pd\n\n# read the data\ndf_sales = pd.read_csv(\"data/sales_train_evaluation.csv\")\ndf_cal = pd.read_csv(\"data/calendar.csv\")\n\nstore_id_set = df_sales.store_id.unique()\n\nidcols = [\n    \"id\",\n    \"item_id\",\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n]\n\n# function to split data by store\n# and convert wide format to long format\n\n\ndef process_store(\n    store_id,\n):\n    print(store_id, \"START\")\n    df_store = df_sales[df_sales.store_id == store_id]\n    df_ts = pd.melt(\n        df_store, id_vars=idcols, value_name=\"sales\"\n    )\n    df_ts = df_ts.merge(\n        df_cal[[\"date\", \"d\"]],\n        left_on=\"variable\",\n        right_on=\"d\",\n        how=\"left\",\n    )\n    df_ts[[\"id\", \"date\", \"sales\"]].to_csv(\n        f\"data/by_store/{store_id.lower()}_target_time_series.csv.gz\",\n        index=False,\n        compression=\"gzip\",\n    )\n\n    print(store_id, \"END\")\n\n\n# trigger the processing\nfrom multiprocessing import Pool\nfrom itertools import product\n\nwith Pool(processes=6) as p:\n    p.starmap(process_store, product(store_id_set))\n\n# process the meta data\ndf_meta = (\n    df_sales[idcols]\n    .drop_duplicates()\n    .drop([\"store_id\", \"state_id\", \"item_id\"], axis=1)\n)\ndf_meta.to_csv(\n    f\"data/by_store/m5_item_meta.csv\", index=False\n)\n</code></pre>"},{"location":"tutorials/m5/#run-forecasts","title":"Run Forecasts","text":"<p>Once we have the data split by store, we can build a model for each store-level dataset.</p> <pre><code>import pandas as pd\nimport requests\nimport os.path\n\n\"\"\"\nUpload files to s3\n\n!aws s3 sync data/by_store/ s3://convect-data/m5/by_store/\n\"\"\"\n\nstores = [\n    \"ca_1\",\n    \"ca_2\",\n    \"ca_3\",\n    \"ca_4\",\n    \"tx_1\",\n    \"tx_2\",\n    \"tx_3\",\n    \"wi_1\",\n    \"wi_2\",\n    \"wi_3\",\n]\nendpoint = \"https://forecast.convect.ai/api/\"\n\n# helper functions\ndef get_ts_path(store_id):\n    return f\"s3://convect-data/m5/by_store/{store_id.lower()}_target_time_series.csv.gz\"\n\n\ndef get_meta_path():\n    return f\"s3://convect-data/m5/by_store/m5_item_meta.csv\"\n\n\ndef create_data_group(store_id):\n    url = os.path.join(endpoint, \"data-groups/\")\n\n    payload = {\"name\": f\"{store_id}-data-groups\"}\n\n    url = os.path.join(endpoint, \"data-groups/\")\n\n    resp = requests.post(url, json=payload)\n\n    resp_payload = resp.json()\n\n    return resp_payload[\"id\"]\n\n\ndef create_ts_dataset(store_id, data_group_id):\n    url = os.path.join(endpoint, \"datasets/\")\n\n    payload = {\n        \"name\": f\"target_time_series_{store_id}\",\n        \"dataset_type\": \"TARGET_TIME_SERIES\",\n        \"path\": get_ts_path(store_id),\n        \"file_format\": \"csv\",\n        \"frequency\": \"D\",\n        \"data_group\": data_group_id,\n        \"schemas\": [\n            {\"name\": \"id\", \"col_type\": \"key\"},\n            {\"name\": \"date\", \"col_type\": \"time\"},\n            {\"name\": \"sales\", \"col_type\": \"num\"},\n        ],\n    }\n\n    resp = requests.post(url, json=payload)\n    resp_payload = resp.json()\n    print(resp_payload)\n\n    return resp_payload[\"id\"]\n\n\ndef create_meta_dataset(store_id, data_group_id):\n    url = os.path.join(endpoint, \"datasets/\")\n\n    payload = {\n        \"name\": f\"item_meta_{store_id}\",\n        \"dataset_type\": \"ITEM_METADATA\",\n        \"path\": get_meta_path(),\n        \"file_format\": \"csv\",\n        \"data_group\": data_group_id,\n        \"schemas\": [\n            {\"name\": \"id\", \"col_type\": \"key\"},\n            {\"name\": \"dept_id\", \"col_type\": \"str\"},\n            {\"name\": \"cat_id\", \"col_type\": \"str\"},\n        ],\n    }\n\n    resp = requests.post(url, json=payload)\n    resp_payload = resp.json()\n    print(resp_payload)\n\n    return resp_payload[\"id\"]\n\n\ndef create_predictor_config(store_id, data_group_id):\n    url = os.path.join(endpoint, \"predictor-configs/\")\n\n    payload = {\n        \"name\": f\"validation-forecast-config-{store_id}\",\n        \"result_uri\": f\"s3://convect-data/test-data/m5-api-result/{store_id}/\",\n        \"horizon\": 28,\n        \"frequency\": \"D\",\n        \"data_group\": data_group_id,\n    }\n\n    res = requests.post(url, json=payload)\n    resp_payload = res.json()\n    print(resp_payload)\n\n    return resp_payload[\"id\"]\n\n\ndef create_predictor(config_id):\n    url = os.path.join(endpoint, \"predictors/\")\n\n    payload = {\"predictor_config\": config_id}\n\n    res = requests.post(url, json=payload)\n    resp_payload = res.json()\n    print(resp_payload)\n\n    return resp_payload[\"id\"]\n\n\ndef query_predictor_status(predictor_id):\n    url = os.path.join(endpoint, \"predictors/\")\n\n    res = requests.get(os.path.join(url, str(predictor_id)))\n\n    return res.json()\n\n\ndef start_prediction_by_api(store_id):\n    data_group_id = create_data_group(store_id)\n    _ = create_ts_dataset(store_id, data_group_id)\n    _ = create_meta_dataset(store_id, data_group_id)\n    config_id = create_predictor_config(\n        store_id, data_group_id\n    )\n    predictor_id = create_predictor(config_id)\n    return query_predictor_status(predictor_id)\n\n\n# trigger the prediction\nfor store in stores:\n    start_prediction_by_api(store)\n</code></pre>"},{"location":"tutorials/m5/#combine-results","title":"Combine Results","text":"<p>Once all the models finish, we can read and combine all the store level results &amp; post-process them in the format ready for submission to Kaggle.</p> <pre><code>import pandas as pd\n\nstores = [\n    \"ca_1\",\n    \"ca_2\",\n    \"ca_3\",\n    \"ca_4\",\n    \"tx_1\",\n    \"tx_2\",\n    \"tx_3\",\n    \"wi_1\",\n    \"wi_2\",\n    \"wi_3\",\n]\n\n\n# combine the resuls\nbase_out_path = \"s3://convect-data/test-data/m5-api-result/by_store/{}/data\"\n\n\ndfs = []\nfor store in stores:\n    dfs.append(pd.read_csv(base_out_path.format(store)))\ndf_pred = pd.concat(dfs, axis=0).drop(\n    [\"Unnamed: 0\"], axis=1\n)\n\ndf_pred[\"dt_label\"] = pd.to_datetime(\n    df_pred[\"predict_start_date\"]\n) - pd.to_datetime(\"2016-04-24\")\ndf_pred[\"dt_label\"] = \"F\" + df_pred[\n    \"dt_label\"\n].dt.days.astype(str)\ndf_pred[\"id\"] = df_pred[\"id\"].map(\n    lambda x: x + \"_\" + \"evaluation\"\n)\n\n# long to wide format\ndf_wide = df_pred[[\"id\", \"predict_sum\", \"dt_label\"]].pivot(\n    index=\"id\", columns=\"dt_label\", values=\"predict_sum\"\n)\n\n# reorder the columns\ncols = list(map(lambda x: f\"F{x}\", range(1, 29)))\ndf_wide = df_wide[cols]\ndf_wide.to_csv(\"submission.csv\")\n</code></pre>"}]}